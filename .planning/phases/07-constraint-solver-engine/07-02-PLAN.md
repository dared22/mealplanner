---
phase: 07-constraint-solver-engine
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - Backend/fastapi_app/main.py
  - Backend/fastapi_app/planner.py
autonomous: true

must_haves:
  truths:
    - "Users with 10+ ratings receive solver-generated plans"
    - "Users with <10 ratings receive OpenAI-generated plans"
    - "Solver fallback triggers for timeout, quality threshold, or infeasibility"
    - "Fallback is silent to user - they get a valid plan either way"
    - "Solver fallback events are logged in activity_logs"
    - "Impossible constraints return error to user (no plan, no fallback)"
  artifacts:
    - path: "Backend/fastapi_app/main.py"
      provides: "Hybrid plan generation routing"
      contains: "_should_use_solver"
    - path: "Backend/fastapi_app/planner.py"
      provides: "generate_daily_plan with solver integration"
      contains: "generate_personalized_plan"
  key_links:
    - from: "Backend/fastapi_app/main.py"
      to: "solver.py"
      via: "conditional import for solver generation"
      pattern: "from solver import"
    - from: "Backend/fastapi_app/planner.py"
      to: "main.py"
      via: "generate_daily_plan called by background task"
      pattern: "generate_daily_plan_for_preference"
---

<objective>
Wire the constraint solver into the meal plan generation flow with hybrid routing (solver for 10+ ratings, OpenAI otherwise) and automatic fallback.

Purpose: Enable personalized plans while maintaining reliability through seamless OpenAI fallback
Output: Modified main.py and planner.py that route users to solver or OpenAI based on rating count
</objective>

<execution_context>
@/Users/pasha/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pasha/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-constraint-solver-engine/07-CONTEXT.md
@.planning/phases/07-constraint-solver-engine/07-01-SUMMARY.md
@Backend/fastapi_app/main.py
@Backend/fastapi_app/planner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add solver routing to plan generation</name>
  <files>Backend/fastapi_app/main.py</files>
  <action>
Modify `_generate_plan_in_background` to use solver for users with 10+ ratings.

1. **Add helper to check solver eligibility:**
```python
def _should_use_solver(db: Session, user_id: UUID) -> bool:
    """Check if user has enough ratings for solver-based generation."""
    rating_count = db.scalar(
        select(func.count(Rating.id)).where(Rating.user_id == user_id)
    ) or 0
    return rating_count >= 10  # Personalization threshold
```

2. **Add helper to check impossible constraints:**
```python
def _is_impossible_constraint(preference: Preference, macro_goal: Dict[str, Any]) -> Optional[str]:
    """
    Check if user's constraints are mathematically impossible.
    Returns error message if impossible, None otherwise.

    Checks:
    - Vegan + high protein (>30% of calories from protein)
    - Very low calorie (<1000 cal/day)
    - Multiple severe restrictions (vegan + nut-free + soy-free)
    - High protein + low calorie combination
    """
    # Get macro targets
    targets = macro_goal.get("macroTargets", {}) if macro_goal else {}
    calories = macro_goal.get("calorieTarget", 2000) if macro_goal else 2000
    protein = targets.get("protein", 0)
    dietary = preference.dietary_restrictions or []

    # Check 1: Very low calorie (unrealistic for any diet)
    if calories < 1000:
        return (
            "Your calorie target is too low for healthy meal planning. "
            "Please set a target of at least 1000 calories per day."
        )

    # Check 2: Vegan high protein (max realistic ~30% from protein)
    # Protein has 4 cal/g, so max protein_g = (calories * 0.30) / 4
    if "vegan" in dietary:
        max_vegan_protein = (calories * 0.30) / 4
        if protein > max_vegan_protein * 1.2:  # 20% tolerance
            return (
                f"Your goals may be incompatible: {protein}g protein on a vegan diet "
                f"with {calories} calories is very difficult to achieve. "
                "Please adjust your protein target or dietary restrictions."
            )

    # Check 3: High protein + low calorie (any diet)
    # Max sustainable is ~35% of calories from protein
    max_protein_any_diet = (calories * 0.35) / 4
    if protein > max_protein_any_diet * 1.2:
        return (
            f"Your protein target ({protein}g) is very high for {calories} calories. "
            "Please increase calories or reduce protein target."
        )

    # Check 4: Multiple severe restrictions (limited recipe pool)
    severe_restrictions = {"vegan", "gluten_free", "nut_free", "soy_free"}
    active_severe = [r for r in dietary if r in severe_restrictions]
    if len(active_severe) >= 3:
        return (
            f"You have multiple dietary restrictions ({', '.join(active_severe)}) "
            "that significantly limit available recipes. Please consider relaxing "
            "one restriction to get better meal variety."
        )

    return None
```

3. **Modify `_generate_plan_in_background` to add solver routing:**

This function currently has an existing `_run_with_timeout` helper for OpenAI generation. The solver path needs to be added alongside it, not replace it.

After getting the preference, before calling `generate_daily_plan_for_preference`:

```python
def _generate_plan_in_background(pref_id: int) -> None:
    """
    Background task for plan generation.

    Integration with existing code:
    - Keep existing _run_with_timeout wrapper for OpenAI path
    - Add solver path that runs BEFORE falling back to _run_with_timeout
    - Both paths use _persist_plan_result to store results
    """
    db = SessionLocal()
    try:
        preference = db.get(Preference, pref_id)
        if preference is None:
            logger.warning("Preference %s missing when generating plan", pref_id)
            return

        user_id = preference.user_id
        use_solver = user_id and _should_use_solver(db, user_id)

        # Check for impossible constraints first (both paths)
        macro_response = generate_daily_macro_goal(preference)
        macro_goal = macro_response.get("goal")

        impossible_error = _is_impossible_constraint(preference, macro_goal)
        if impossible_error:
            plan_result = {"plan": None, "error": impossible_error}
            _persist_plan_result(db, preference, plan_result)
            log_activity(
                db,
                actor_type="user",
                actor_id=user_id,
                action_type="plan_generation",
                action_detail=impossible_error,
                status="error",
                metadata={"preference_id": pref_id, "reason": "impossible_constraints"},
            )
            return

        if use_solver:
            # Try solver first, falls back to OpenAI internally
            plan_result = _generate_solver_plan(db, user_id, preference)
            _persist_plan_result(db, preference, plan_result, generation_source="solver")
        else:
            # Use existing OpenAI path with timeout wrapper
            plan_result = _run_with_timeout(
                lambda: generate_daily_plan_for_preference(preference, db=db),
                timeout_seconds=PLAN_GENERATION_TIMEOUT
            )
            _persist_plan_result(db, preference, plan_result, generation_source="openai")

        # ... rest of existing code (activity logging) ...
```

4. **Add solver generation with fallback:**

```python
def _generate_solver_plan(db: Session, user_id: UUID, preference: Preference) -> Dict[str, Any]:
    """Generate plan using solver with automatic OpenAI fallback."""
    from solver import generate_personalized_plan

    try:
        solver_result = generate_personalized_plan(
            db=db,
            user_id=user_id,
            preference=preference,
            timeout_seconds=10,  # Solver timeout (OpenAI fallback adds ~5s buffer)
        )

        # Check if solver succeeded
        if solver_result.get("plan") is not None:
            fallback_reason = solver_result.get("fallback_reason")
            if fallback_reason:
                # Solver completed but quality was poor - log and fallback
                log_activity(
                    db,
                    actor_type="system",
                    action_type="solver_fallback",
                    action_detail=f"Solver fallback: {fallback_reason}",
                    status="warning",
                    metadata={
                        "user_id": str(user_id),
                        "reason": fallback_reason,
                        "quality_metrics": solver_result.get("quality_metrics"),
                    },
                )
                # Fall through to OpenAI
            else:
                # Solver succeeded with good quality
                return solver_result

        # Solver failed or poor quality - try OpenAI
        fallback_reason = solver_result.get("fallback_reason") or solver_result.get("error") or "unknown"
        log_activity(
            db,
            actor_type="system",
            action_type="solver_fallback",
            action_detail=f"Using OpenAI fallback: {fallback_reason}",
            status="warning",
            metadata={"user_id": str(user_id), "reason": fallback_reason},
        )

    except Exception as exc:
        logger.exception("Solver failed with exception, falling back to OpenAI")
        log_activity(
            db,
            actor_type="system",
            action_type="solver_fallback",
            action_detail=f"Solver exception: {exc}",
            status="error",
            metadata={"user_id": str(user_id), "exception": str(exc)},
        )

    # OpenAI fallback
    return generate_daily_plan(preference, translate=False, db=db)
```

5. **Update imports at top of main.py:**
```python
from planner import generate_daily_plan, generate_daily_macro_goal, generate_daily_plan_for_preference
from activity_logging import log_activity  # Import activity logging helper
from sqlalchemy import func, select  # For rating count query
```

Note: `log_activity` is the existing activity logging function from the activity-logging phase. Verify the exact import path from the activity_logging module or inline in main.py.
  </action>
  <verify>
Review the code changes:
- `_should_use_solver` returns True for users with 10+ ratings
- `_generate_plan_in_background` routes to solver or OpenAI
- Fallback events are logged with activity_logs
- Impossible constraints return error (no plan generated)
  </verify>
  <done>
- Hybrid routing implemented: 10+ ratings -> solver, <10 -> OpenAI
- Solver fallback logs to activity_logs for admin monitoring
- Impossible constraints return error message to user
  </done>
</task>

<task type="auto">
  <name>Task 2: Add plan generation source tracking</name>
  <files>Backend/fastapi_app/main.py</files>
  <action>
Track whether a plan was generated by solver or OpenAI for transparency and debugging.

1. **Modify `_persist_plan_result` to accept source parameter:**

```python
def _persist_plan_result(
    db: Session,
    preference: Preference,
    plan_result: Dict[str, Any],
    generation_source: str = "openai",  # "solver" or "openai"
) -> None:
    existing_raw = preference.raw_data if isinstance(preference.raw_data, dict) else {}
    updated_raw = dict(existing_raw)

    # Add generation metadata
    plan_result_with_meta = dict(plan_result)
    plan_result_with_meta["generation_source"] = generation_source
    plan_result_with_meta["generated_at"] = datetime.now(timezone.utc).isoformat()

    updated_raw["generated_plan"] = _json_safe(plan_result_with_meta)
    # ... rest of existing code ...
```

2. **Update calls to `_persist_plan_result` to pass source:**

In `_generate_solver_plan`:
- Success: `_persist_plan_result(db, preference, plan_result, generation_source="solver")`
- Fallback: `_persist_plan_result(db, preference, openai_result, generation_source="openai_fallback")`

In original OpenAI path:
- `_persist_plan_result(db, preference, plan_result, generation_source="openai")`

3. **Expose generation_source in GET /preferences/{pref_id} response:**

Add to response dict:
```python
generation_source = None
if isinstance(generated_plan, dict):
    generation_source = generated_plan.get("generation_source")

return {
    # ... existing fields ...
    "generation_source": generation_source,  # "solver", "openai", or "openai_fallback"
}
```

This allows:
- Admin monitoring of solver adoption rate
- Debugging when users report issues
- Phase 8 UI can optionally show "Personalized Plan" indicator
  </action>
  <verify>
Test the preference endpoint response includes generation_source field
  </verify>
  <done>
- Plan results include generation_source metadata
- GET /preferences/{id} returns generation_source
- Admin can track solver vs OpenAI usage patterns
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Code review checklist:
   - [ ] `_should_use_solver` checks rating count >= 10
   - [ ] Solver fallback logs to activity_logs with metadata
   - [ ] Impossible constraints return error (not fallback)
   - [ ] `generation_source` tracked in raw_data
   - [ ] GET /preferences/{id} returns generation_source

2. Flow verification:
   - User with <10 ratings -> OpenAI path (existing behavior)
   - User with 10+ ratings -> solver path -> fallback to OpenAI if needed
   - Impossible constraints -> error message, no plan
</verification>

<success_criteria>
- Hybrid routing works based on rating count threshold
- Solver failures silently fall back to OpenAI
- Activity logs capture fallback events for admin monitoring
- Impossible constraints surface error to user
- Generation source is tracked and exposed in API
</success_criteria>

<output>
After completion, create `.planning/phases/07-constraint-solver-engine/07-02-SUMMARY.md`
</output>
